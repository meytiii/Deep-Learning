{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import the necessary libraries**"
      ],
      "metadata": {
        "id": "a0sDpJ6EIOta"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "import shutil"
      ],
      "metadata": {
        "id": "JTmxm9AiIRc-"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set the seed value for experiment reproducibility.**"
      ],
      "metadata": {
        "id": "beqZwCxMISlz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed = 42\n",
        "tf.random.set_seed(seed)\n",
        "np.random.seed(seed)"
      ],
      "metadata": {
        "id": "R8g6ag75IUb3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download and extract the dataset from Tensorflow's Mini Speech Commands dataset**"
      ],
      "metadata": {
        "id": "fUayoUhRIW7V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_PATH = '/content/mini_speech_commands'\n",
        "#Since im using colab for this, the path is set like this,\n",
        "#feel free to change the path if you're running on your own PC.\n",
        "if not os.path.exists(DATASET_PATH):\n",
        "    tf.keras.utils.get_file(\n",
        "        'mini_speech_commands.zip',\n",
        "        origin=\"http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\",\n",
        "        extract=True,\n",
        "        cache_dir='/content/',\n",
        "        cache_subdir='.')"
      ],
      "metadata": {
        "id": "Kzs58VIhIWOo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "829bcefe-8e4e-4a23-ecc3-fc48065c3efb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip\n",
            "182082353/182082353 [==============================] - 1s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Since we are only working with yes,go and left in this example, im gonna remove other folders so we get less processing time.**"
      ],
      "metadata": {
        "id": "drjAAKVfKDzn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "commands = ['yes', 'go', 'left']\n",
        "for folder in os.listdir(DATASET_PATH):\n",
        "    folder_path = os.path.join(DATASET_PATH, folder)\n",
        "    if os.path.isdir(folder_path) and folder not in commands:\n",
        "        shutil.rmtree(folder_path)"
      ],
      "metadata": {
        "id": "GQypnIf-ItlY"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess the audio data**"
      ],
      "metadata": {
        "id": "pmG_wTCPIvcF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_audio(audio, labels):\n",
        "    audio = tf.squeeze(audio, axis=-1)\n",
        "    return audio, labels"
      ],
      "metadata": {
        "id": "J9VAHpxhIw9Y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load and preprocess the training and validation datasets**"
      ],
      "metadata": {
        "id": "OgKwnTtvI2fD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds, val_ds = tf.keras.utils.audio_dataset_from_directory(\n",
        "    directory=DATASET_PATH,\n",
        "    batch_size=64,\n",
        "    validation_split=0.2,\n",
        "    seed=0,\n",
        "    output_sequence_length=16000,\n",
        "    subset='both',\n",
        "    labels='inferred',\n",
        "    label_mode='int',\n",
        ")\n",
        "\n",
        "train_ds = train_ds.map(preprocess_audio, tf.data.AUTOTUNE)\n",
        "val_ds = val_ds.map(preprocess_audio, tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "u9OHA90rI38m",
        "outputId": "8a4fe72c-ffd7-4915-a838-c485a94602b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3000 files belonging to 3 classes.\n",
            "Using 2400 files for training.\n",
            "Using 600 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now time to define the model architecture**\n",
        "\n",
        "*   We want the inputs in shapes of 16000 items.\n",
        "*   Activation is set to 'relu'\n",
        "\n"
      ],
      "metadata": {
        "id": "ZoI1jzPCI89v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (16000,)\n",
        "num_labels = len(commands)\n",
        "\n",
        "model = models.Sequential([\n",
        "    layers.Input(shape=input_shape),\n",
        "    layers.Reshape((16000, 1)),\n",
        "    layers.Conv1D(32, 3, activation='relu'),\n",
        "    layers.Conv1D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling1D(),\n",
        "    layers.Dropout(0.25),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dropout(0.5),\n",
        "    layers.Dense(num_labels),\n",
        "])"
      ],
      "metadata": {
        "id": "m1yTfSi4I8f8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Compiling and then training the model.**"
      ],
      "metadata": {
        "id": "xv1-wlO1JMbB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=['accuracy'],\n",
        ")"
      ],
      "metadata": {
        "id": "Y0546tC4JQtg"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 10\n",
        "history = model.fit(\n",
        "    train_ds,\n",
        "    validation_data=val_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=tf.keras.callbacks.EarlyStopping(verbose=1, patience=2),\n",
        ")"
      ],
      "metadata": {
        "id": "izBPjRZ3JSIA",
        "outputId": "122fd04d-ae6b-407d-c46e-cfcae4662942",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "38/38 [==============================] - 12s 127ms/step - loss: 1.0432 - accuracy: 0.4762 - val_loss: 0.9577 - val_accuracy: 0.4800\n",
            "Epoch 2/10\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.7825 - accuracy: 0.6471 - val_loss: 0.7996 - val_accuracy: 0.6667\n",
            "Epoch 3/10\n",
            "38/38 [==============================] - 4s 96ms/step - loss: 0.6278 - accuracy: 0.7283 - val_loss: 0.7513 - val_accuracy: 0.6900\n",
            "Epoch 4/10\n",
            "38/38 [==============================] - 4s 104ms/step - loss: 0.4952 - accuracy: 0.7962 - val_loss: 0.8521 - val_accuracy: 0.6400\n",
            "Epoch 5/10\n",
            "38/38 [==============================] - 4s 97ms/step - loss: 0.4089 - accuracy: 0.8342 - val_loss: 0.8352 - val_accuracy: 0.7083\n",
            "Epoch 5: early stopping\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the model**\n",
        "\n",
        "*   Finally we are going to evaluate our created model and see the test results.\n",
        "*   This includes the Loss and Accuracy of our model.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nchVKr7kJYyB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test_ds = val_ds.take(1)\n",
        "test_results = model.evaluate(test_ds, return_dict=True)\n",
        "print(\"Test Loss:\", test_results['loss'])\n",
        "print(\"Test Accuracy:\", test_results['accuracy'])"
      ],
      "metadata": {
        "id": "GMtownWhJo6E",
        "outputId": "4ba3da15-b5c0-4642-a879-ab41bc7ec480",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 76ms/step - loss: 1.1192 - accuracy: 0.6719\n",
            "Test Loss: 1.1192364692687988\n",
            "Test Accuracy: 0.671875\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Export the model**"
      ],
      "metadata": {
        "id": "0vWCnu6QJ2OU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can export our created model and use it elsewhere."
      ],
      "metadata": {
        "id": "HyV3sdGGJ6Qr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.save(\"/content/sound_classification_model\")"
      ],
      "metadata": {
        "id": "zaIge7DcJ19l"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "G6HsWkCkU9WK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Test the exported model"
      ],
      "metadata": {
        "id": "pIvNFlqjNQaM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Now that we have exported our model, It's time to load it up in another notebook and test it.**"
      ],
      "metadata": {
        "id": "mEJMx0z1NKCd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the saved model**"
      ],
      "metadata": {
        "id": "SnSDO4mzNX60"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.models.load_model(\"/content/sound_classification_model\")"
      ],
      "metadata": {
        "id": "74UZp-N8NaMw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load an example**\n",
        "\n",
        "*   Here we just copied an example from our own dataset.\n",
        "*   The example is \"Yes\".\n",
        "*   Let's see if our model can detect the voice.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Uhg6JGOZNcPf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "example_file_path = '/content/mini_speech_commands/yes/01648c51_nohash_1.wav' #yes\n",
        "#example_file_path = '/content/mini_speech_commands/left/0e5193e6_nohash_0.wav' #left\n",
        "#example_file_path = '/content/mini_speech_commands/go/07c5129e_nohash_0.wav' #go\n",
        "audio_binary = tf.io.read_file(example_file_path)\n",
        "waveform, _ = tf.audio.decode_wav(audio_binary)"
      ],
      "metadata": {
        "id": "z4c5KHMlNrrC"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess the audio data**"
      ],
      "metadata": {
        "id": "AT7OM3sTNt7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "waveform = tf.squeeze(waveform, axis=-1)\n",
        "waveform = tf.reshape(waveform, (1, -1))"
      ],
      "metadata": {
        "id": "VeU8E4XHNuVp"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Predict the class.**"
      ],
      "metadata": {
        "id": "LxrBwNjlN0PM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(waveform)\n",
        "predicted_class_index = np.argmax(predictions)"
      ],
      "metadata": {
        "id": "DeDEJv4LN2D8",
        "outputId": "6a113006-8afd-4f75-ba08-9863a30dc981",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 18ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**We used these names during training so we are gonna stick with them in here too.**"
      ],
      "metadata": {
        "id": "kPzH8Os6OCdm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = ['go', 'left', 'yes']"
      ],
      "metadata": {
        "id": "fMrq66iaN618"
      },
      "execution_count": 138,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Get the predicted class index**\n",
        "\n",
        "*   Go=0, Left=1, Yes=2\n",
        "\n"
      ],
      "metadata": {
        "id": "V2emQLQGOIE6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class_name = class_names[predicted_class_index]"
      ],
      "metadata": {
        "id": "7Vgy3y6nOAqw"
      },
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**And finally we're gonna print out the predicted class index and name accordingly.**"
      ],
      "metadata": {
        "id": "dkvYTIG6OeUn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Predicted class name:\", predicted_class_name)"
      ],
      "metadata": {
        "id": "dqNkDp2zOX_t",
        "outputId": "7c90da47-09b3-43f6-9878-4511b7aa6a23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class name: yes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feel free to test out other classes.\n",
        "\n",
        "*   For \"Yes\", Change the example_file_path to : '/content/mini_speech_commands/yes/0132a06d_nohash_1.wav'\n",
        "*   For \"Go\", Change the example_file_path to : '/content/mini_speech_commands/go/016e2c6d_nohash_0.wav'\n",
        "*   For \"Left\" Change the example_file_path to : '/content/mini_speech_commands/left/0132a06d_nohash_0.wav'\n",
        "\n"
      ],
      "metadata": {
        "id": "I11djoyaOjSL"
      }
    }
  ]
}