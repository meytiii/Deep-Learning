{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Importing the required libraries**"
      ],
      "metadata": {
        "id": "euEUaCa8ZmiN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "metadata": {
        "id": "Y2yC-kNQZq3h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the dataset**\n"
      ],
      "metadata": {
        "id": "lZyBHsaLZscO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://raw.githubusercontent.com/meytiii/Deep-Learning/main/datasets/6-persian-topics.zip\"\n",
        "dataset = tf.keras.utils.get_file(\"6-persian-topics.zip\", url, extract=True)\n",
        "\n",
        "data_dir = os.path.join(os.path.dirname(dataset), \"6-persian-topics\")\n",
        "#This format of data pathing is mostly used in Google Colab, Feel free to change if you are running this notebook on your own PC."
      ],
      "metadata": {
        "id": "GGQ879FEZwtV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Process the dataset**\n",
        "Seperate labels and texts in their own lists"
      ],
      "metadata": {
        "id": "EPcZS0HFZ39o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class_names = os.listdir(data_dir)\n",
        "class_dirs = [os.path.join(data_dir, class_name) for class_name in class_names]\n",
        "texts = []\n",
        "labels = []\n",
        "\n",
        "for i, class_dir in enumerate(class_dirs):\n",
        "    file_names = os.listdir(class_dir)\n",
        "    for file_name in file_names:\n",
        "        file_path = os.path.join(class_dir, file_name)\n",
        "        with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "            text = file.read()\n",
        "            texts.append(text)\n",
        "            labels.append(i)"
      ],
      "metadata": {
        "id": "frRks5HPZ5XR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenize the text**"
      ],
      "metadata": {
        "id": "s5W_XsVzZ9d3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_features = 10000\n",
        "max_len = 200\n",
        "tokenizer = Tokenizer(num_words=max_features, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "X = pad_sequences(sequences, maxlen=max_len)"
      ],
      "metadata": {
        "id": "Ki7mU9VTZ_Je"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.array(labels)"
      ],
      "metadata": {
        "id": "UoVbt1VwaBiO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split the dataset into training and validation sets**"
      ],
      "metadata": {
        "id": "lEzvZr3jaDlY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "6KAY9KQbaHXw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the model**"
      ],
      "metadata": {
        "id": "cXky-7m4aI5I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(max_features, 16, input_length=max_len),\n",
        "    tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Dense(16, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(len(class_names), activation=\"softmax\")\n",
        "])\n",
        "\n",
        "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
        "              optimizer=\"adam\",\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "FxLkUlR3aKmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model**"
      ],
      "metadata": {
        "id": "Q6nTZokmaOhW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 15\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_data=(X_val, y_val),\n",
        "    epochs=epochs,\n",
        "    verbose=2\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QP8XbwkXaM0P",
        "outputId": "203ad454-235c-4a36-ee35-65ca677d55a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "940/940 - 5s - loss: 0.9692 - accuracy: 0.6729 - val_loss: 0.4705 - val_accuracy: 0.8770 - 5s/epoch - 5ms/step\n",
            "Epoch 2/15\n",
            "940/940 - 6s - loss: 0.3308 - accuracy: 0.8983 - val_loss: 0.2603 - val_accuracy: 0.9084 - 6s/epoch - 6ms/step\n",
            "Epoch 3/15\n",
            "940/940 - 4s - loss: 0.2053 - accuracy: 0.9352 - val_loss: 0.1881 - val_accuracy: 0.9466 - 4s/epoch - 4ms/step\n",
            "Epoch 4/15\n",
            "940/940 - 5s - loss: 0.1495 - accuracy: 0.9550 - val_loss: 0.1598 - val_accuracy: 0.9501 - 5s/epoch - 5ms/step\n",
            "Epoch 5/15\n",
            "940/940 - 6s - loss: 0.1186 - accuracy: 0.9643 - val_loss: 0.1372 - val_accuracy: 0.9581 - 6s/epoch - 7ms/step\n",
            "Epoch 6/15\n",
            "940/940 - 5s - loss: 0.0963 - accuracy: 0.9713 - val_loss: 0.1254 - val_accuracy: 0.9623 - 5s/epoch - 5ms/step\n",
            "Epoch 7/15\n",
            "940/940 - 6s - loss: 0.0796 - accuracy: 0.9760 - val_loss: 0.1162 - val_accuracy: 0.9662 - 6s/epoch - 7ms/step\n",
            "Epoch 8/15\n",
            "940/940 - 5s - loss: 0.0666 - accuracy: 0.9804 - val_loss: 0.1119 - val_accuracy: 0.9671 - 5s/epoch - 5ms/step\n",
            "Epoch 9/15\n",
            "940/940 - 5s - loss: 0.0561 - accuracy: 0.9847 - val_loss: 0.1072 - val_accuracy: 0.9697 - 5s/epoch - 5ms/step\n",
            "Epoch 10/15\n",
            "940/940 - 5s - loss: 0.0479 - accuracy: 0.9871 - val_loss: 0.1035 - val_accuracy: 0.9706 - 5s/epoch - 5ms/step\n",
            "Epoch 11/15\n",
            "940/940 - 4s - loss: 0.0416 - accuracy: 0.9890 - val_loss: 0.1022 - val_accuracy: 0.9718 - 4s/epoch - 4ms/step\n",
            "Epoch 12/15\n",
            "940/940 - 5s - loss: 0.0352 - accuracy: 0.9910 - val_loss: 0.1026 - val_accuracy: 0.9723 - 5s/epoch - 6ms/step\n",
            "Epoch 13/15\n",
            "940/940 - 4s - loss: 0.0308 - accuracy: 0.9922 - val_loss: 0.1017 - val_accuracy: 0.9730 - 4s/epoch - 4ms/step\n",
            "Epoch 14/15\n",
            "940/940 - 4s - loss: 0.0277 - accuracy: 0.9932 - val_loss: 0.1041 - val_accuracy: 0.9741 - 4s/epoch - 5ms/step\n",
            "Epoch 15/15\n",
            "940/940 - 6s - loss: 0.0248 - accuracy: 0.9937 - val_loss: 0.1047 - val_accuracy: 0.9755 - 6s/epoch - 6ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Evaluate the model and get important informations like F1-Score**"
      ],
      "metadata": {
        "id": "NCHYYDYoaRyd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(X_val)\n",
        "y_pred = [np.argmax(pred) for pred in y_pred]\n",
        "print(classification_report(y_val, y_pred, target_names=class_names))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5YdBeFiaTO-",
        "outputId": "248a221a-2579-43ce-9e2f-2aade5323a51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "235/235 [==============================] - 0s 2ms/step\n",
            "                                  precision    recall  f1-score   support\n",
            "\n",
            "فناوری و علوم کاربردی و تکنولوژی       0.95      0.96      0.96      1322\n",
            "                جغرافیا و مکانها       0.99      0.99      0.99      4334\n",
            "                  بهداشت و سلامت       0.92      0.97      0.94       381\n",
            "                    دین و اعتقاد       0.88      0.77      0.82       115\n",
            "                            ورزش       1.00      0.99      0.99      1295\n",
            "                         ریاضیات       0.63      0.50      0.56        68\n",
            "\n",
            "                        accuracy                           0.98      7515\n",
            "                       macro avg       0.89      0.86      0.88      7515\n",
            "                    weighted avg       0.97      0.98      0.98      7515\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As you can see, Each F1-Score for every subject is above 0.5 which is a good thing.**"
      ],
      "metadata": {
        "id": "kidvaqYkaeYX"
      }
    }
  ]
}
