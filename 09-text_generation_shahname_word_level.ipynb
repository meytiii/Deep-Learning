{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Import necessary libraries**"
      ],
      "metadata": {
        "id": "AQS6SJ-sI53o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from tensorflow import keras\n",
        "import re"
      ],
      "metadata": {
        "id": "6m_fUuSzI4mD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/meytiii/Deep-Learning/main/datasets/shahname.csv -O shahname.csv"
      ],
      "metadata": {
        "id": "SofIQSYJI8DX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('shahname.csv')\n",
        "text = '\\n'.join(df['Text'])\n",
        "# Display the first 250 characters of the text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "id": "ow3QDmVCI-XC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Preprocess the text: tokenization into words**"
      ],
      "metadata": {
        "id": "FlM2I7gHJA0u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
        "words = text.split(' ')\n",
        "print(f'Total words: {len(words)}')\n"
      ],
      "metadata": {
        "id": "wFdLZOHEJABx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a vocabulary of unique words**"
      ],
      "metadata": {
        "id": "L8IMDEzBJFLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(words))\n",
        "print(f'{len(vocab)} unique words')\n",
        "for v in vocab[:100]:  # Display the first 100 words\n",
        "    print(f'{v}', end=' ')\n",
        "\n",
        "# Mapping from words to IDs and vice versa\n",
        "ids_to_words = keras.layers.StringLookup(vocabulary=vocab, invert=True, mask_token=None)\n",
        "ids_from_words = keras.layers.StringLookup(vocabulary=vocab, invert=False, mask_token=None)"
      ],
      "metadata": {
        "id": "K79aPnHtJEjy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for v in ids_from_words.get_vocabulary()[:100]:  # Display the first 100 words\n",
        "    print(v, end=' ')"
      ],
      "metadata": {
        "id": "-KRfoMvtJJZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert text to sequence of IDs**"
      ],
      "metadata": {
        "id": "PqkU08ZaJL2w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_words(tf.strings.split(text))\n",
        "all_ids = tf.squeeze(all_ids)  # Ensure the shape is correct\n",
        "print(all_ids[:20])  # Display the first 20 IDs"
      ],
      "metadata": {
        "id": "xBZlV3ucJK8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define sequence length and batch size**"
      ],
      "metadata": {
        "id": "MeTzkj6yJSMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 20  # Since we are working at the word level, we can use a smaller sequence length\n",
        "BATCH_SIZE = 64\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "metadata": {
        "id": "J-CQ-MVwJQ_N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a dataset of sequences**"
      ],
      "metadata": {
        "id": "tb0F8lSnJXiz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "sequences = ids_dataset.batch(SEQUENCE_LENGTH + 1, drop_remainder=True, num_parallel_calls=AUTOTUNE)"
      ],
      "metadata": {
        "id": "WXocI3dAJYjW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split input and target texts**"
      ],
      "metadata": {
        "id": "SN5cjUAMJcCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target, num_parallel_calls=AUTOTUNE)\n",
        "\n",
        "# Create training batches\n",
        "dataset = dataset.batch(BATCH_SIZE, num_parallel_calls=AUTOTUNE, drop_remainder=True)\n",
        "dataset = dataset.prefetch(AUTOTUNE)"
      ],
      "metadata": {
        "id": "BE-94f27DHOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Creating and compiling our model**"
      ],
      "metadata": {
        "id": "TcmLx8uAJhvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model parameters\n",
        "VOCAB_SIZE = len(ids_from_words.get_vocabulary())\n",
        "EMBEDDING_DIM = 256\n",
        "RNN_UNITS = 1024\n",
        "\n",
        "# Define the model\n",
        "class MyModel(keras.Model):\n",
        "    def __init__(self, vocabulary_size, embedding_dim, rnn_units):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.embedding = keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\n",
        "        self.gru = keras.layers.GRU(units=rnn_units, return_sequences=True, return_state=True)\n",
        "        self.dense = keras.layers.Dense(vocabulary_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = self.embedding(inputs, training=training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        return x\n",
        "\n",
        "model = MyModel(vocabulary_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, rnn_units=RNN_UNITS)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "# Set up checkpoints\n",
        "checkpoints_dir = './temp/chpts/'\n",
        "checkpoint_prefix = os.path.join(checkpoints_dir, 'chpt_{epoch}')\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)\n",
        "\n",
        "# Train the model\n",
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "id": "F_wmIPV0JlDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating text and testing our model**"
      ],
      "metadata": {
        "id": "-VgPcwb3JoIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a class for generating text\n",
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, ids_to_words, ids_from_words, temperature=1.0):\n",
        "        super(OneStep, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.ids_to_words = ids_to_words\n",
        "        self.ids_from_words = ids_from_words\n",
        "        self.vocab_size = ids_from_words.vocabulary_size()\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        # Expand dims to match the expected shape: (batch_size, sequence_length)\n",
        "        inputs = tf.expand_dims(inputs, 0)  # from (sequence_length,) to (1, sequence_length)\n",
        "\n",
        "        predicted_logits, states = self.model(inputs, states=states, return_state=True)\n",
        "        predicted_logits = predicted_logits[:, -1, :]  # get the last time step's output\n",
        "        predicted_logits = predicted_logits / self.temperature\n",
        "\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "        predicted_words = self.ids_to_words(predicted_ids)\n",
        "        return predicted_words, states\n",
        "\n",
        "# Generate text\n",
        "one_step_model = OneStep(model, ids_to_words, ids_from_words, temperature=1.2)  # Adjust temperature\n",
        "\n",
        "# Start seed text\n",
        "seed_text = \"به نام خداوند جان و خرد\"\n",
        "seed_ids = ids_from_words(tf.strings.split(seed_text))\n",
        "\n",
        "# Number of words to generate\n",
        "num_generate = 100\n",
        "\n",
        "# Poem structure parameters\n",
        "line_length = 5  # Number of words per line\n",
        "num_lines = 5    # Number of lines per stanza\n",
        "num_stanzas = 4  # Number of stanzas\n",
        "\n",
        "# Generate the poem\n",
        "generated_poem = seed_text\n",
        "generated_poem += '\\n'  # Start a new line after the seed text\n",
        "\n",
        "states = None\n",
        "next_input = seed_ids\n",
        "word_count = 0\n",
        "\n",
        "for _ in range(num_generate):\n",
        "    next_word, states = one_step_model.generate_one_step(next_input, states=states)\n",
        "    next_word = next_word.numpy()[0].decode('utf-8')  # Convert to string\n",
        "    generated_poem += ' ' + next_word\n",
        "    word_count += 1\n",
        "\n",
        "    # Insert a line break after every `line_length` words\n",
        "    if word_count % line_length == 0:\n",
        "        generated_poem += '\\n'\n",
        "\n",
        "    # Insert a stanza break after every `num_lines` lines\n",
        "    if word_count % (line_length * num_lines) == 0:\n",
        "        generated_poem += '\\n'\n",
        "\n",
        "    next_input = tf.expand_dims(ids_from_words(tf.constant(next_word)), 0)\n",
        "\n",
        "print(generated_poem)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kK1gmrHGDPnF",
        "outputId": "e544d679-789e-4b1e-f9fe-2a3ea0a40d55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "به نام خداوند جان و خرد\n",
            " بر دژم مر آن را\n",
            " جفا را برادر بود خرد\n",
            " را زن و منم بر\n",
            " گناه فزون بود سر چون\n",
            " جهان پهلوان بزرگان هم از\n",
            "\n",
            " دین این هر دو آشکار\n",
            " سپر بر سر و افسر\n",
            " و ترگ و مهر به\n",
            " خوبی هر آن نامه را\n",
            " خوب چهر پس پشت این\n",
            "\n",
            " شاه برتر ز مهر برآورد\n",
            " تخت جهاندار آگنده شاه نهان\n",
            " گشت پیدا ز تخت بود\n",
            " یکی گم ز این سپس\n",
            " با بهشت همان تخت فرخ\n",
            "\n",
            " سرو سیمین پدیدار هر چه\n",
            " فرمان کنید برو نیز با\n",
            " خود بهم پر ز روشن\n",
            " کند اگر یار باشم به\n",
            " باغ اندرون همان نیز گوهر\n",
            "\n",
            "\n"
          ]
        }
      ]
    }
  ]
}