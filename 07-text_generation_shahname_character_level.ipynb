{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/meytiii/Deep-Learning/main/datasets/shahname.csv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NNG_uVorMw-a",
        "outputId": "7851d124-5857-44d4-aab2-e0ba77ab57d1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-13 11:47:11--  https://raw.githubusercontent.com/meytiii/Deep-Learning/main/datasets/shahname.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5609688 (5.3M) [text/plain]\n",
            "Saving to: ‘shahname.csv’\n",
            "\n",
            "shahname.csv        100%[===================>]   5.35M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2024-06-13 11:47:12 (146 MB/s) - ‘shahname.csv’ saved [5609688/5609688]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "from tensorflow import keras"
      ],
      "metadata": {
        "id": "kbvMUiChMnrN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Load the dataset**"
      ],
      "metadata": {
        "id": "pb2DUPD-aFac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/shahname.csv')\n",
        "text = '\\n'.join(df['Text'])"
      ],
      "metadata": {
        "id": "yX-3mkesaExZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display the first 250 characters of the text**"
      ],
      "metadata": {
        "id": "Fx8CVlzSaIwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmD46jB9aH20",
        "outputId": "ec9a6919-5851-49dc-ff8a-5878b9e6344d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "به نام خداوند جان و خرد\n",
            "کز این برتر اندیشه بر نگذرد\n",
            "خداوند نام و خداوند جای\n",
            "خداوند روزی ده رهنمای\n",
            "خداوند کیوان و گَردان سپهر\n",
            "فروزندهٔ ماه و ناهید و مهر\n",
            "ز نام و نشان و گمان برتر است\n",
            "نگارندهٔ بر شده پیکر است\n",
            "به بینندگان آفریننده را\n",
            "نبینی مرنجان دو بینن\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a vocabulary of unique characters**"
      ],
      "metadata": {
        "id": "Is0UI9JCaLvI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')\n",
        "for v in vocab:\n",
        "    print(f'{v}', end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dzkDV1YaK8i",
        "outputId": "94e8caf7-0aac-40ae-a847-e7c76c2769f1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "57 unique characters\n",
            "\n",
            "   ! ( ) :   « » ، ؛ ؟ ء آ أ ؤ ئ ا ب ت ث ج ح خ د ذ ر ز س ش ص ض ط ظ ع غ ف ق ل م ن ه و ي َ ُ ِ ّ ْ ٔ پ چ ژ ک گ ی ‌ "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mapping from characters to IDs and vice versa**"
      ],
      "metadata": {
        "id": "gJARZh6WaPft"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_to_chars = keras.layers.StringLookup(vocabulary=vocab, invert=True, mask_token=None)\n",
        "ids_from_chars = keras.layers.StringLookup(vocabulary=vocab, invert=False, mask_token=None)"
      ],
      "metadata": {
        "id": "BH7FYg0yaPOe"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Display the vocabulary**"
      ],
      "metadata": {
        "id": "EFzcAT58aTFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for v in ids_from_chars.get_vocabulary():\n",
        "    print(v, end=' ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dn-dFr3MaS8p",
        "outputId": "d9b313d2-b6c1-4631-8010-6cdb414d2914"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[UNK] \n",
            "   ! ( ) :   « » ، ؛ ؟ ء آ أ ؤ ئ ا ب ت ث ج ح خ د ذ ر ز س ش ص ض ط ظ ع غ ف ق ل م ن ه و ي َ ُ ِ ّ ْ ٔ پ چ ژ ک گ ی ‌ "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Convert text to sequence of IDs**"
      ],
      "metadata": {
        "id": "O5LTeAaxaYK-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))"
      ],
      "metadata": {
        "id": "s4c9NwnQaXfP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define sequence length and batch size**"
      ],
      "metadata": {
        "id": "xI-rT0goacjR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "SEQUENCE_LENGTH = 100\n",
        "BATCH_SIZE = 64\n",
        "AUTOTUNE = tf.data.experimental.AUTOTUNE"
      ],
      "metadata": {
        "id": "y3HFvjR1ab5Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create a dataset of sequences**"
      ],
      "metadata": {
        "id": "KsWkFPreaffg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)\n",
        "sequences = ids_dataset.batch(SEQUENCE_LENGTH + 1, drop_remainder=True, num_parallel_calls=AUTOTUNE)"
      ],
      "metadata": {
        "id": "RymAF6PEafVZ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Split input and target texts**"
      ],
      "metadata": {
        "id": "escOW2iHah8k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target, num_parallel_calls=AUTOTUNE)"
      ],
      "metadata": {
        "id": "yUzGYsMcahxj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Create training batches**"
      ],
      "metadata": {
        "id": "tcjvJ-dRalCn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.batch(BATCH_SIZE, num_parallel_calls=AUTOTUNE, drop_remainder=True)\n",
        "dataset = dataset.prefetch(AUTOTUNE)"
      ],
      "metadata": {
        "id": "l_rsdfG8aktw"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Model parameters**"
      ],
      "metadata": {
        "id": "AbxQZKnJanz6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = len(ids_from_chars.get_vocabulary())\n",
        "EMBEDDING_DIM = 10\n",
        "RNN_UNITS = 2048"
      ],
      "metadata": {
        "id": "TUQmg7cFans_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define the model**"
      ],
      "metadata": {
        "id": "ZRXByRk3aq0w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MyModel(keras.Model):\n",
        "    def __init__(self, vocabulary_size, embedding_dim, rnn_units):\n",
        "        super(MyModel, self).__init__()\n",
        "        self.embedding = keras.layers.Embedding(input_dim=vocabulary_size, output_dim=embedding_dim)\n",
        "        self.gru = keras.layers.GRU(units=rnn_units, return_sequences=True, return_state=True)\n",
        "        self.dense = keras.layers.Dense(vocabulary_size)\n",
        "\n",
        "    def call(self, inputs, states=None, return_state=False, training=False):\n",
        "        x = inputs\n",
        "        x = self.embedding(x, training=training)\n",
        "        if states is None:\n",
        "            states = self.gru.get_initial_state(x)\n",
        "        x, states = self.gru(x, initial_state=states, training=training)\n",
        "        x = self.dense(x, training=training)\n",
        "        if return_state:\n",
        "            return x, states\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ev3C3ULJaqau"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MyModel(vocabulary_size=VOCAB_SIZE, embedding_dim=EMBEDDING_DIM, rnn_units=RNN_UNITS)\n",
        "\n",
        "sample_input = tf.random.uniform((BATCH_SIZE, SEQUENCE_LENGTH), dtype=tf.int32, minval=0, maxval=VOCAB_SIZE)\n",
        "model(sample_input)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EAwYb-lOaub7",
        "outputId": "eca942fb-a00c-4b08-cdbe-9211b181095b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(64, 100, 58), dtype=float32, numpy=\n",
              "array([[[-5.7661737e-04, -5.2061817e-04, -8.8580680e-04, ...,\n",
              "          5.2249507e-04,  1.1439670e-03, -7.4360811e-04],\n",
              "        [-3.1005214e-03, -1.9589346e-03, -3.2850323e-04, ...,\n",
              "         -3.6103535e-04, -4.3967448e-04, -1.9227804e-03],\n",
              "        [-2.5325555e-03,  3.5561589e-04, -1.7043864e-03, ...,\n",
              "          6.0410355e-05, -2.1175449e-03,  1.2615696e-04],\n",
              "        ...,\n",
              "        [-1.2644095e-03,  1.7504464e-03, -1.1788621e-03, ...,\n",
              "         -1.0697353e-03,  4.8672577e-05, -1.7204704e-03],\n",
              "        [-1.3111050e-03,  5.0596666e-04, -3.7897439e-04, ...,\n",
              "          1.2282966e-04,  1.4894943e-03,  7.2462717e-04],\n",
              "        [ 7.6025550e-04, -2.1489503e-04, -1.2188213e-04, ...,\n",
              "         -1.9392715e-04,  1.2229050e-03,  2.0593975e-03]],\n",
              "\n",
              "       [[-3.3872670e-03, -1.7113626e-03,  1.6196291e-03, ...,\n",
              "         -4.9119152e-04,  2.0570228e-04, -2.4463406e-03],\n",
              "        [-1.0136575e-03, -1.6698710e-04, -1.1979711e-05, ...,\n",
              "         -5.9060310e-04,  7.5249176e-04, -2.7714628e-03],\n",
              "        [-1.0110635e-03,  1.1598766e-03, -5.0113298e-04, ...,\n",
              "         -9.4078679e-04,  1.5415845e-04, -2.3195269e-03],\n",
              "        ...,\n",
              "        [ 1.6065438e-03,  4.6638658e-04,  9.5049274e-04, ...,\n",
              "          1.1128810e-03, -1.6779281e-04,  7.9058507e-04],\n",
              "        [ 1.5059118e-03,  3.0763983e-03, -5.2624225e-04, ...,\n",
              "          3.6908704e-04, -2.4529421e-03,  1.2039009e-03],\n",
              "        [-1.5708216e-04,  1.9723882e-03, -1.0933611e-03, ...,\n",
              "         -6.4469868e-04, -1.3807961e-03, -8.4926537e-04]],\n",
              "\n",
              "       [[ 6.6848256e-04,  1.5088986e-03, -1.2810567e-03, ...,\n",
              "         -3.0336092e-04, -1.0988578e-03,  1.8163061e-03],\n",
              "        [ 8.2294282e-04,  1.6434502e-03, -3.8756852e-03, ...,\n",
              "         -7.8329275e-04, -1.2352723e-03,  8.4129104e-04],\n",
              "        [-1.4739889e-03, -4.7267476e-04, -1.1296503e-03, ...,\n",
              "          2.3158162e-04, -2.8067079e-04, -1.2411169e-03],\n",
              "        ...,\n",
              "        [-1.0552139e-03,  1.3636325e-03, -1.4216093e-03, ...,\n",
              "         -6.6262612e-04, -7.0079148e-04, -1.5922866e-03],\n",
              "        [-2.2425016e-03,  5.8287068e-04, -2.0001337e-03, ...,\n",
              "         -9.7336958e-04, -1.9463354e-04,  4.5585723e-04],\n",
              "        [-2.4476624e-03, -1.5598004e-03,  3.1276088e-04, ...,\n",
              "         -5.1534350e-04,  2.4176179e-03, -2.5730167e-04]],\n",
              "\n",
              "       ...,\n",
              "\n",
              "       [[-3.0075200e-03, -1.8667299e-03,  1.5861039e-04, ...,\n",
              "         -3.8282521e-04, -8.2288892e-04, -1.5491673e-03],\n",
              "        [-3.0319155e-03, -2.6211306e-03,  1.0926615e-03, ...,\n",
              "          1.9006140e-05,  2.0202775e-03, -1.0420602e-03],\n",
              "        [-1.5699735e-03,  1.3482699e-04, -1.1970535e-03, ...,\n",
              "          2.5608642e-05, -1.4305612e-03, -1.1766831e-03],\n",
              "        ...,\n",
              "        [-1.4711553e-03,  2.5219976e-03, -1.5401279e-03, ...,\n",
              "          5.2891136e-04, -3.1469488e-03,  1.0450529e-03],\n",
              "        [-3.4286850e-04,  4.0857876e-03, -1.8786368e-03, ...,\n",
              "         -1.3091888e-04, -3.8137494e-03,  1.1403938e-03],\n",
              "        [ 6.1159942e-04,  4.0644072e-03, -3.1577675e-03, ...,\n",
              "         -1.6226009e-03, -1.6934410e-03,  1.4224669e-03]],\n",
              "\n",
              "       [[ 1.7021879e-03,  7.5698190e-04,  1.4902438e-03, ...,\n",
              "          8.3743309e-04, -5.0208543e-04,  1.1876896e-03],\n",
              "        [ 2.1609538e-03,  2.1421928e-03,  5.7393726e-04, ...,\n",
              "          1.0549424e-03, -2.2198656e-03, -1.9556662e-04],\n",
              "        [-2.9194489e-05, -7.6960830e-04,  1.9131443e-03, ...,\n",
              "          3.8192561e-04,  1.8034335e-03, -5.2309362e-04],\n",
              "        ...,\n",
              "        [ 1.1732893e-03, -4.6161585e-04, -5.9736479e-04, ...,\n",
              "          1.4107587e-04,  6.1951473e-04,  5.7784171e-05],\n",
              "        [-1.3954823e-03,  5.0875032e-04, -1.1357374e-03, ...,\n",
              "         -9.1016956e-04, -2.7227355e-03, -1.3566217e-03],\n",
              "        [-9.0526138e-04,  2.7909613e-04, -7.8975072e-04, ...,\n",
              "         -1.5688518e-03, -2.2833236e-03, -2.0114479e-03]],\n",
              "\n",
              "       [[-8.8342105e-04,  5.5951066e-04, -1.0600311e-03, ...,\n",
              "         -9.1065874e-04, -5.1118404e-04, -1.2730828e-03],\n",
              "        [-1.5663820e-03,  7.7897822e-04, -1.5773454e-03, ...,\n",
              "         -1.4175852e-03, -6.7286420e-04, -1.9818109e-03],\n",
              "        [-1.7430852e-03, -2.2742283e-04, -1.6811846e-03, ...,\n",
              "         -2.7861085e-04,  9.4391580e-04, -1.8196704e-03],\n",
              "        ...,\n",
              "        [-3.2142769e-03, -5.5865385e-04, -2.4803746e-03, ...,\n",
              "         -1.2384293e-03, -1.4626946e-03, -1.6490918e-03],\n",
              "        [-1.2490549e-03, -7.1753247e-04, -2.5428424e-03, ...,\n",
              "          7.5769145e-05, -3.8551558e-03, -1.6150796e-03],\n",
              "        [ 8.3565689e-04,  2.2164561e-04, -3.3251750e-03, ...,\n",
              "         -1.2622890e-03, -2.2946412e-03,  2.1947458e-04]]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BX4ML7G9azZh",
        "outputId": "4af07427-1915-4a6e-a286-2ff4b73336a8"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  580       \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  12656640  \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  118842    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 12776062 (48.74 MB)\n",
            "Trainable params: 12776062 (48.74 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True))"
      ],
      "metadata": {
        "id": "_Ws0eicPa0pX"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Set up checkpoints**"
      ],
      "metadata": {
        "id": "CLrFzdINa213"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoints_dir = './temp/chpts/'\n",
        "checkpoint_prefix = os.path.join(checkpoints_dir, 'chpt_{epoch}')\n",
        "checkpoint_callback = keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix, save_weights_only=True)"
      ],
      "metadata": {
        "id": "q5twBwopa2Gv"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Train the model**"
      ],
      "metadata": {
        "id": "fCugx4Yna6JD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EPOCHS = 30\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lcacgdmKa42q",
        "outputId": "275838bc-9af3-4d2c-d8c2-1ea0860c0b4b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "396/396 [==============================] - 74s 179ms/step - loss: 2.6356\n",
            "Epoch 2/30\n",
            "396/396 [==============================] - 72s 182ms/step - loss: 1.9355\n",
            "Epoch 3/30\n",
            "396/396 [==============================] - 72s 183ms/step - loss: 1.6278\n",
            "Epoch 4/30\n",
            "396/396 [==============================] - 72s 183ms/step - loss: 1.4654\n",
            "Epoch 5/30\n",
            "396/396 [==============================] - 72s 183ms/step - loss: 1.3530\n",
            "Epoch 6/30\n",
            "396/396 [==============================] - 73s 184ms/step - loss: 1.2590\n",
            "Epoch 7/30\n",
            "396/396 [==============================] - 72s 183ms/step - loss: 1.1661\n",
            "Epoch 8/30\n",
            "396/396 [==============================] - 72s 183ms/step - loss: 1.0665\n",
            "Epoch 9/30\n",
            "396/396 [==============================] - 72s 183ms/step - loss: 0.9716\n",
            "Epoch 10/30\n",
            "396/396 [==============================] - 73s 184ms/step - loss: 0.9135\n",
            "Epoch 11/30\n",
            "396/396 [==============================] - 72s 183ms/step - loss: 0.8789\n",
            "Epoch 12/30\n",
            "396/396 [==============================] - 72s 183ms/step - loss: 0.8375\n",
            "Epoch 13/30\n",
            "396/396 [==============================] - 72s 183ms/step - loss: 0.7984\n",
            "Epoch 14/30\n",
            "396/396 [==============================] - 73s 183ms/step - loss: 0.7696\n",
            "Epoch 15/30\n",
            "396/396 [==============================] - 73s 183ms/step - loss: 0.7516\n",
            "Epoch 16/30\n",
            "396/396 [==============================] - 73s 184ms/step - loss: 0.7334\n",
            "Epoch 17/30\n",
            "396/396 [==============================] - 73s 184ms/step - loss: 0.7192\n",
            "Epoch 18/30\n",
            "396/396 [==============================] - 72s 182ms/step - loss: 0.7124\n",
            "Epoch 19/30\n",
            "396/396 [==============================] - 73s 184ms/step - loss: 0.7048\n",
            "Epoch 20/30\n",
            "396/396 [==============================] - 72s 183ms/step - loss: 0.7040\n",
            "Epoch 21/30\n",
            "396/396 [==============================] - 73s 184ms/step - loss: 0.6989\n",
            "Epoch 22/30\n",
            "396/396 [==============================] - 73s 184ms/step - loss: 0.6937\n",
            "Epoch 23/30\n",
            "396/396 [==============================] - 73s 184ms/step - loss: 0.6927\n",
            "Epoch 24/30\n",
            "396/396 [==============================] - 73s 184ms/step - loss: 0.6941\n",
            "Epoch 25/30\n",
            "396/396 [==============================] - 73s 183ms/step - loss: 0.7009\n",
            "Epoch 26/30\n",
            "396/396 [==============================] - 73s 183ms/step - loss: 0.6992\n",
            "Epoch 27/30\n",
            "396/396 [==============================] - 73s 184ms/step - loss: 0.6934\n",
            "Epoch 28/30\n",
            "396/396 [==============================] - 73s 184ms/step - loss: 0.6962\n",
            "Epoch 29/30\n",
            "396/396 [==============================] - 72s 183ms/step - loss: 0.6971\n",
            "Epoch 30/30\n",
            "396/396 [==============================] - 73s 183ms/step - loss: 0.6977\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Define a class for generating text**"
      ],
      "metadata": {
        "id": "mgcwXmila9ZP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class OneStep(tf.keras.Model):\n",
        "    def __init__(self, model, ids_to_chars, ids_from_chars, temperature=1.0):\n",
        "        super(OneStep, self).__init__()\n",
        "        self.temperature = temperature\n",
        "        self.model = model\n",
        "        self.ids_to_chars = ids_to_chars\n",
        "        self.ids_from_chars = ids_from_chars\n",
        "\n",
        "        skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "        sparse_mask = tf.SparseTensor(\n",
        "            values=[-float('inf')] * len(skip_ids),\n",
        "            indices=skip_ids,\n",
        "            dense_shape=[len(ids_from_chars.get_vocabulary())]\n",
        "        )\n",
        "        self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "    @tf.function\n",
        "    def generate_one_step(self, inputs, states=None):\n",
        "        input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "        input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "        predicted_logits, states = self.model(inputs=input_ids, states=states, return_state=True)\n",
        "        predicted_logits = predicted_logits[:, -1, :] / self.temperature\n",
        "        predicted_logits += self.prediction_mask\n",
        "        predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "        predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "        predicted_chars = self.ids_to_chars(predicted_ids)\n",
        "        return predicted_chars, states"
      ],
      "metadata": {
        "id": "NkQT9QoDa8mI"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generate and display text**"
      ],
      "metadata": {
        "id": "aKTFbk_IbAX5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "one_step_model = OneStep(model, ids_to_chars, ids_from_chars, temperature=0.75)\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['به نام خداوند'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "    next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "    result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "\n",
        "# Display generated text and runtime\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n\\n')\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tqQ9f2xbABS",
        "outputId": "c309edbf-c4e6-43de-9832-be9f67774f57"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "به نام خداوند جای\n",
            "یکی نامه‌ای بن ببین جایگاه\n",
            "سپهبد چو بشنید دینار او\n",
            "ز گلبرگشاهن گذشت ارجمند\n",
            "به پیش سراپرده‌ام بنده‌ایم\n",
            "که فرزند من زاد فرزند خویش\n",
            "کجا هم به بی‌تان و تیغی ودی\n",
            "بتار کمان و به سوی نشی\n",
            "تو من در هله خیره دینار دفت\n",
            "برین گونه یاری  زو یاد کرد\n",
            "ز جایی که گفتند ما بی‌هرربه سرد\n",
            "شود جوشن و ترگ پر آرده دروی\n",
            "که این مهره را تا ده و دو هزار\n",
            "سساپند و گردنکشی داده‌ایم\n",
            "دل از باستان پیش خسرو دوید\n",
            "کمان را بزه کرد و دل کرده دید\n",
            "همه کاخ و از تو پر آب گشت\n",
            "پییده شدی پیش و پیل دهم\n",
            "یکی مهتری بر در شهریار\n",
            "فرستاد و در شهر نه مردِ بود\n",
            "سخن هرچه پرسیم کاووس کن\n",
            "بزرگان و فرزند روشن توبر\n",
            "نه آن رفتگان را ز تخم پدر\n",
            "که چون او منم سایهٔ فر تو\n",
            "بباشیم تا نام تو سر به چرخ\n",
            "به بالین نهاد آن گرامی  زود\n",
            "که خیزد پر از آب رومه به مخت\n",
            "همه یک بیک اندر آورد شاد\n",
            "در گنج بد شاد و بالا گرفت\n",
            "سپهبد چو برخواند آن توبگ تست\n",
            "سوی خیره زان دید برگشت شاه\n",
            "دلیری صمین سی هنرمار شاه\n",
            "ز قلب سپاه اندر آمد به جای\n",
            "چو پیروز بازخ سپیده دمان\n",
            "همی تیر بارید گودرز خاک\n",
            "همی‌داد چون کاخ او خور شاه\n",
            "اگر بد سلیحان نه آیین بدند\n",
            "دگر ناز روز به آن شاختر\n",
            "بدو گفت کای مر \n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Run time: 4.291474342346191\n"
          ]
        }
      ]
    }
  ]
}